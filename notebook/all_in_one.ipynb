{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 0. Prerequisites",
   "id": "65967fd1a0e0656"
  },
  {
   "metadata": {
    "id": "9d8a2900b501ae6e"
   },
   "cell_type": "markdown",
   "source": "## Hardware Information",
   "id": "9d8a2900b501ae6e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "!lscpu",
   "id": "159365b4de49e4ad",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "id": "25313e8b2699aa36"
   },
   "cell_type": "code",
   "source": [
    "!nvidia-smi"
   ],
   "id": "25313e8b2699aa36",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "id": "ba5f519164add311"
   },
   "cell_type": "code",
   "source": [
    "!nvcc -V"
   ],
   "id": "ba5f519164add311",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Environment Variables",
   "id": "3753414e8fd5a1d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# TODO: Check out again while using kokkos\n",
    "!export OMP_NUM_THREADS=4\n",
    "!export OMP_PROC_BIND=spread\n",
    "!export OMP_PLACES=threads"
   ],
   "id": "53a902a900e6f50a",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "id": "f61cd00351455937"
   },
   "cell_type": "markdown",
   "source": "## Install Dependencies",
   "id": "f61cd00351455937"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "USE_GPU = False",
   "id": "458c47c441617f27",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Basic Libraries\n",
    "%pip install tqdm\n",
    "%pip install torch torchvision torchaudio tensorboard\n",
    "%pip install numpy scipy scikit-learn matplotlib pandas\n",
    "%pip install pennylane --upgrade\n",
    "%pip install pennylane-lightning\n",
    "%pip install pennylane-lightning-kokkos\n",
    "%pip install pennylane-qulacs[\"cpu\"]\n",
    "%pip install pyquafu"
   ],
   "metadata": {
    "id": "CZg9qlluZD9d"
   },
   "id": "CZg9qlluZD9d",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if USE_GPU:\n",
    "    %pip install -v --no-cache-dir cuquantum-python-cu12\n",
    "    %pip install nvidia-cuda-cupti-cu12 == 12.1.105\n",
    "    %pip install nvidia-cuda-nvrtc-cu12 == 12.1.105\n",
    "    %pip install nvidia-cudnn-cu12 == 8.9.2.26\n",
    "    %pip install nvidia-cufft-cu12 == 11.0.2.54\n",
    "    %pip install nvidia-curand-cu12 == 10.3.2.106\n",
    "    %pip install nvidia-cusolver-cu12 == 11.4.5.107\n",
    "    %pip install nvidia-nccl-cu12 == 2.19.3\n",
    "    %pip install nvidia-nvtx-cu12 == 12.1.105\n",
    "    %pip install nvidia-cusparse-cu12 == 12.1.0.106\n",
    "    %pip install nvidia-cublas-cu12 == 12.1.3.1\n",
    "    %pip install nvidia-cuda-runtime-cu12 == 12.1.105\n",
    "    # %pip install nvidia-cusparse-cu12\n",
    "    # %pip install nvidia-cublas-cu12\n",
    "    # %pip install nvidia-cuda-runtime-cu12\n",
    "    %pip install custatevec_cu12\n",
    "    %pip install pennylane-lightning[gpu]\n",
    "    # %pip install pennylane-qulacs[\"gpu\"]"
   ],
   "id": "17acfe466c075540",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 1. Import Libraries",
   "id": "57a39f71761a08ce"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import json\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from collections.abc import Iterable\n",
    "from functools import wraps\n",
    "from typing import Any, Callable, Dict, Optional, Type, Union, List, Tuple\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pennylane as qml\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "from quafu import QuantumCircuit, User, Task, simulate\n",
    "from sklearn.metrics import recall_score, f1_score, confusion_matrix\n",
    "from torch import Tensor\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision.transforms import functional as TF\n",
    "from tqdm.notebook import tqdm"
   ],
   "id": "e8ad519db9a27c21",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 2. Quantum Convolutional Components",
   "id": "45f6a937f70d04a9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2.1 Quantum Convolutional Kernel",
   "id": "63e880cf4222eebb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "ArrayLike = Union[list, np.ndarray, torch.Tensor]\n",
    "\n",
    "\n",
    "class QKernel:\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            quantum_channels: int,\n",
    "            kernel_size: Union[int, Tuple[int, int]] = 2,\n",
    "            num_param_blocks: int = 2,\n",
    "            kernel_circuit: Callable[[ArrayLike], None] = None,\n",
    "            weight_shapes: Dict[str, Tuple[int, ...]] = None,\n",
    "    ):\n",
    "        \"\"\"Quantum Kernel\"\"\"\n",
    "        self.validate_params(quantum_channels, kernel_size, num_param_blocks, kernel_circuit, weight_shapes)\n",
    "\n",
    "        self.kernel_size = kernel_size if isinstance(kernel_size, tuple) else (kernel_size, kernel_size)\n",
    "        self.num_qubits = quantum_channels * self.kernel_size[0] * self.kernel_size[1]\n",
    "        self.num_param_blocks = num_param_blocks\n",
    "\n",
    "        self.circuit = kernel_circuit if kernel_circuit else self._default_circuit\n",
    "        self.weight_shapes = weight_shapes if weight_shapes else {\"weights\": (num_param_blocks, 2 * self.num_qubits)}\n",
    "\n",
    "    @staticmethod\n",
    "    def validate_params(\n",
    "            quantum_channels: int,\n",
    "            kernel_size: Union[int, Tuple[int, int]],\n",
    "            num_param_blocks: int,\n",
    "            kernel_circuit: Callable[[ArrayLike], None],\n",
    "            weight_shapes: dict[str, tuple]\n",
    "    ):\n",
    "        if not isinstance(quantum_channels, int) or quantum_channels <= 0:\n",
    "            raise ValueError(\"quantum_channels must be a positive integer\")\n",
    "        if (not isinstance(kernel_size, (int, tuple)) or\n",
    "                (isinstance(kernel_size, int) and kernel_size <= 0) or\n",
    "                (isinstance(kernel_size, tuple) and any(size <= 0 for size in kernel_size))):\n",
    "            raise ValueError(\"kernel_size must be a positive integer or a tuple of positive integers\")\n",
    "        if not isinstance(num_param_blocks, int) or num_param_blocks <= 0:\n",
    "            raise ValueError(\"num_param_blocks must be a positive integer\")\n",
    "        if kernel_circuit and not weight_shapes:\n",
    "            raise ValueError(\"Must provide weight_shapes for custom kernel circuit\")\n",
    "\n",
    "    def _default_circuit(self, inputs: ArrayLike, weights: ArrayLike):\n",
    "        # Encoding Layer\n",
    "        for qubit in range(self.num_qubits):\n",
    "            qml.Hadamard(wires=qubit)\n",
    "            qml.RY(inputs[qubit], wires=qubit)\n",
    "\n",
    "        # Parametric Layer\n",
    "        for layer in range(self.num_param_blocks):\n",
    "            # Entanglement\n",
    "            for qubit in range(self.num_qubits):\n",
    "                qml.CRZ(weights[layer, qubit], wires=[qubit, (qubit + 1) % self.num_qubits])\n",
    "            # Rotation\n",
    "            for qubit in range(self.num_qubits):\n",
    "                qml.RY(weights[layer, self.num_qubits + qubit], wires=qubit)\n",
    "\n",
    "        # Observation Layer\n",
    "        _expectations = [qml.expval(qml.PauliZ(wires=qubit)) for qubit in range(self.num_qubits)]\n",
    "        return _expectations\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"QKernel(num_qubits={self.num_qubits}, kernel_size={self.kernel_size}, num_param_blocks={self.num_param_blocks})\"\n"
   ],
   "id": "21dabb8dbcf5b275",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2.2 Quantum Convolutional Layer",
   "id": "516bac075fa459ad"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class _QuanvNd(nn.Module):\n",
    "    __constants__ = [\"in_channels\", \"out_channels\", \"kernel_size\", \"stride\", \"padding\", \"num_qlayers\", \"qdevice\",\n",
    "                     \"diff_method\"]\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        pass\n"
   ],
   "id": "1d6d11edd85a0147",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class Quanv2d(_QuanvNd):\n",
    "\n",
    "    def __init__(self,\n",
    "                 in_channels: int,\n",
    "                 out_channels: int,\n",
    "                 kernel_size: int = 2,\n",
    "                 stride: int = 1,\n",
    "                 padding: int = 0,\n",
    "                 qkernel: QKernel = None,\n",
    "                 num_qlayers: int = 2,\n",
    "                 qdevice: str = \"default.qubit\",\n",
    "                 qdevice_kwargs: dict = None,\n",
    "                 diff_method: str = \"best\"):\n",
    "        super(Quanv2d, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "\n",
    "        self.qkernel = qkernel or QKernel(quantum_channels=in_channels, kernel_size=kernel_size,\n",
    "                                          num_param_blocks=num_qlayers)\n",
    "        self.qdevice_kwargs = qdevice_kwargs or {}\n",
    "        self.qdevice = qml.device(qdevice, wires=self.qkernel.num_qubits, **self.qdevice_kwargs)\n",
    "        self.qnode = qml.QNode(self.qkernel.circuit, device=self.qdevice, interface=\"torch\", diff_method=diff_method)\n",
    "        self.qlayer = qml.qnn.TorchLayer(self.qnode, self.qkernel.weight_shapes)\n",
    "\n",
    "        # Use 1x1 classical convolution to match the desired output channels\n",
    "        self.classical_conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Verified params\n",
    "        assert len(x.shape) == 4, \"Input tensor must have 4 dimensions: (batch_size, channels, height, width)\"\n",
    "        assert x.dtype == torch.float32, \"Input tensor must have dtype torch.float32\"\n",
    "        assert x.shape[1] == self.in_channels, f\"Input tensor must have {self.in_channels} input channels\"\n",
    "\n",
    "        # Apply quantum convolution\n",
    "        x = self.quantum_conv(x)\n",
    "\n",
    "        # Apply 1x1 classical convolution to match the desired output channels\n",
    "        x = self.classical_conv(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def quantum_conv(self, x):\n",
    "        bs, _, h, w = x.shape\n",
    "\n",
    "        # Apply padding to the input tensor\n",
    "        if self.padding != 0:\n",
    "            x = F.pad(x, (self.padding,) * 4, mode=\"constant\", value=0)\n",
    "            h += 2 * self.padding\n",
    "            w += 2 * self.padding\n",
    "\n",
    "        # Unfold the input tensor to extract overlapping patches\n",
    "        patches = F.unfold(x, kernel_size=self.kernel_size, stride=self.stride, padding=self.padding)\n",
    "        patches = patches.permute(0, 2, 1)  # Reshape to (bs, num_patches, in_channels * kernel_size^2)\n",
    "\n",
    "        # Apply quantum kernel to x\n",
    "        out = []\n",
    "        # Apply quantum layer to each batch\n",
    "        for i in range(bs):\n",
    "            batch_out = [self.qlayer(patch) for patch in patches[i]]\n",
    "            out.append(torch.stack(batch_out))\n",
    "        out = torch.stack(out)\n",
    "\n",
    "        # Fold the output tensor\n",
    "        out = out.permute(0, 2, 1)\n",
    "        out = F.fold(out, output_size=(h, w), kernel_size=self.kernel_size, stride=self.stride, padding=self.padding)\n",
    "\n",
    "        # Normalization\n",
    "        ones = torch.ones_like(x)\n",
    "        unfolded_ones = F.unfold(ones, kernel_size=self.kernel_size, stride=self.stride, padding=self.padding)\n",
    "        folded_ones = F.fold(unfolded_ones, output_size=(h, w), kernel_size=self.kernel_size, stride=self.stride,\n",
    "                             padding=self.padding)\n",
    "        out = out / folded_ones\n",
    "\n",
    "        return out\n"
   ],
   "id": "f6d415abfbcc40d9",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2.3 Quafu Extensions",
   "id": "12aa47b72d73755e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2.3.1 Quafu Convolutional Kernel",
   "id": "aa6d20dc20ef6227"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class QuafuQKernel:\n",
    "\n",
    "    def __init__(self,\n",
    "                 in_channels: int,\n",
    "                 kernel_size: int = 2,\n",
    "                 num_qlayers: int = 2,\n",
    "                 kernel_circuit: Callable[[ArrayLike], None] = None,\n",
    "                 weight_shapes: dict[str, tuple] = None,\n",
    "                 qdevice=\"simulator\",\n",
    "                 api_token: str = \"\"):\n",
    "        \"\"\"Quafu Quantum Kernel\"\"\"\n",
    "        self.validate_params(in_channels, kernel_size, num_qlayers, kernel_circuit, weight_shapes)\n",
    "\n",
    "        self.num_qubits = in_channels * kernel_size ** 2\n",
    "        self.num_qlayers = num_qlayers\n",
    "\n",
    "        self.circuit = kernel_circuit if kernel_circuit else self._default_circuit\n",
    "        self.weight_shapes = weight_shapes if weight_shapes else {\"weights\": (num_qlayers, 2 * self.num_qubits)}\n",
    "\n",
    "        self.qdevice = qdevice\n",
    "        self.api_token = api_token\n",
    "\n",
    "        self.qcircuit = None\n",
    "        if qdevice != \"simulator\":\n",
    "            self.task = Task()\n",
    "            user = User(self.api_token)\n",
    "            user.save_apitoken()\n",
    "            self.task.config(backend=self.qdevice, shots=2000, compile=True)\n",
    "\n",
    "    @staticmethod\n",
    "    def validate_params(in_channels, kernel_size, num_qlayers, kernel_circuit, weight_shapes):\n",
    "        if not isinstance(in_channels, int) or in_channels <= 0:\n",
    "            raise ValueError(\"in_channels must be a positive integer\")\n",
    "        if not isinstance(kernel_size, int) or kernel_size <= 0:\n",
    "            raise ValueError(\"kernel_size must be a positive integer\")\n",
    "        if not isinstance(num_qlayers, int) or num_qlayers <= 0:\n",
    "            raise ValueError(\"num_qlayers must be a positive integer\")\n",
    "        if kernel_circuit and not weight_shapes:\n",
    "            raise ValueError(\"Must provide weight_shapes for custom parametric_layer\")\n",
    "\n",
    "    @staticmethod\n",
    "    def expval_z_expectations(probabilities, num_qubits):\n",
    "        \"\"\"\n",
    "        Calculate the expectation values of the Z operator for each qubit.\n",
    "        \"\"\"\n",
    "        z_expectations = [0] * num_qubits\n",
    "        probs = probabilities\n",
    "\n",
    "        if not isinstance(probabilities, dict):\n",
    "            bases = [format(i, \"0\" + str(num_qubits) + \"b\") for i in range(2 ** num_qubits)]\n",
    "            probs = dict(zip(bases, probabilities))\n",
    "\n",
    "        for base, prob in probs.items():\n",
    "            for i in range(num_qubits):\n",
    "                # For Z operator: |0> contributes +1, |1> contributes -1\n",
    "                z_expectations[i] += prob * (1 if base[i] == \"0\" else -1)\n",
    "\n",
    "        return z_expectations\n",
    "\n",
    "    def _default_circuit(self, inputs: ArrayLike, weights: ArrayLike):\n",
    "        # Quantum Kernel Circuit\n",
    "        self.qcircuit = QuantumCircuit(self.num_qubits)\n",
    "\n",
    "        # TODO: Check Data Type(It seems that np.ndarray is supported with warning and torch.Tensor is not supported.)\n",
    "        if isinstance(inputs, (np.ndarray, torch.Tensor)):\n",
    "            inputs = inputs.tolist()\n",
    "        if isinstance(weights, (np.ndarray, torch.Tensor)):\n",
    "            weights = weights.tolist()\n",
    "\n",
    "        # Encoding Layer\n",
    "        for qubit in range(self.num_qubits):\n",
    "            self.qcircuit.h(qubit)\n",
    "            self.qcircuit.ry(qubit, inputs[qubit])\n",
    "\n",
    "        # Parametric Layer\n",
    "        for layer in range(self.num_qlayers):\n",
    "            # Entanglement\n",
    "            for i in range(self.num_qubits):\n",
    "                # 分解 CRZ = RZ + CNOT + RZ + CNOT\n",
    "                # q_0: ─────────────■────────────────■──\n",
    "                #      ┌─────────┐┌─┴─┐┌──────────┐┌─┴─┐\n",
    "                # q_1: ┤ Rz(λ/2) ├┤ X ├┤ Rz(-λ/2) ├┤ X ├\n",
    "                #      └─────────┘└───┘└──────────┘└───┘\n",
    "                self.qcircuit.rz((i + 1) % self.num_qubits, weights[layer][i] / 2)\n",
    "                self.qcircuit.cx(i, (i + 1) % self.num_qubits)\n",
    "                self.qcircuit.rz((i + 1) % self.num_qubits, -weights[layer][i] / 2)\n",
    "                self.qcircuit.cx(i, (i + 1) % self.num_qubits)\n",
    "            # Rotation\n",
    "            for qubit in range(self.num_qubits):\n",
    "                self.qcircuit.ry(qubit, weights[layer][self.num_qubits + qubit])\n",
    "\n",
    "        # Observation Layer\n",
    "        self.qcircuit.measure()\n",
    "\n",
    "        if self.qdevice == \"simulator\":\n",
    "            results = simulate(self.qcircuit, output=\"probabilities\").probabilities\n",
    "        else:\n",
    "            results = self.task.send(self.qcircuit, wait=True).probabilities\n",
    "\n",
    "        _expectations = self.expval_z_expectations(results, self.num_qubits)\n",
    "\n",
    "        return _expectations\n"
   ],
   "id": "e3ac962d0ce718b7",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2.3.2 QuafuTorchLayer",
   "id": "bfffaf3a14ae00cc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class QuafuTorchLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, qkernel: QuafuQKernel, weight_shapes: dict):\n",
    "        super().__init__()\n",
    "        weight_shapes = {\n",
    "            weight: (\n",
    "                tuple(size)\n",
    "                if isinstance(size, Iterable)\n",
    "                else () if size == 1 else (size,)\n",
    "            )\n",
    "            for weight, size in weight_shapes.items()\n",
    "        }\n",
    "        # validate the QNode signature, and convert to a Torch QNode.\n",
    "        self.qkernel = qkernel\n",
    "        self.circuit = qkernel.circuit\n",
    "        self.qnode_weights: Dict[str, torch.nn.Parameter] = {}\n",
    "        self._init_weights(weight_shapes=weight_shapes)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"Evaluates a forward pass through the QNode based upon input data and the initialized\n",
    "        weights.\n",
    "\n",
    "        Args:\n",
    "            inputs (tensor): data to be processed\n",
    "\n",
    "        Returns:\n",
    "            tensor: output data\n",
    "        \"\"\"\n",
    "        if len(inputs.shape) > 1:\n",
    "            # If the input size is not 1-dimensional, unstack the input along its first dimension,\n",
    "            # recursively call the forward pass on each of the yielded tensors, and then stack the\n",
    "            # outputs back into the correct shape\n",
    "            reconstructor = [self.forward(x) for x in torch.unbind(inputs)]\n",
    "            return torch.stack(reconstructor)\n",
    "\n",
    "        # If the input is 1-dimensional, calculate the forward pass as usual\n",
    "        return self._evaluate_qnode(inputs)\n",
    "\n",
    "    def _evaluate_qnode(self, x):\n",
    "        \"\"\"Evaluates the QNode for a single input datapoint.\n",
    "\n",
    "        Args:\n",
    "            x (tensor): the datapoint\n",
    "\n",
    "        Returns:\n",
    "            tensor: output datapoint\n",
    "        \"\"\"\n",
    "        kwargs = {\"inputs\": x}\n",
    "        kwargs.update({arg: weight.to(x) for arg, weight in self.qnode_weights.items()})\n",
    "\n",
    "        result = torch.tensor(self.circuit(**kwargs))\n",
    "\n",
    "        if isinstance(result, torch.Tensor):\n",
    "            return result.type(x.dtype)\n",
    "\n",
    "        return torch.hstack(result).type(x.dtype)\n",
    "\n",
    "    def _init_weights(self, weight_shapes: Dict[str, tuple]):\n",
    "        \"\"\"Initialize and register the weights, and weights are randomly initialized from the uniform distribution\n",
    "        on the interval [0, 2π].\n",
    "        \"\"\"\n",
    "        for name, size in weight_shapes.items():\n",
    "            weight = torch.Tensor(*size)\n",
    "            torch.nn.init.uniform_(weight, b=2 * math.pi)\n",
    "            self.qnode_weights[name] = torch.nn.Parameter(weight)\n",
    "            self.register_parameter(name, self.qnode_weights[name])\n"
   ],
   "id": "dcc6817dc5aac5d1",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2.3.3 Quafu Convolutional Layer",
   "id": "f1d06421fb0495e3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class QuafuQuanv2d(_QuanvNd):\n",
    "\n",
    "    def __init__(self,\n",
    "                 in_channels: int,\n",
    "                 out_channels: int,\n",
    "                 kernel_size: int = 2,\n",
    "                 stride: int = 1,\n",
    "                 padding: int = 0,\n",
    "                 qkernel: QuafuQKernel = None,\n",
    "                 num_qlayers: int = 2,\n",
    "                 qdevice: str = \"simulator\",\n",
    "                 api_token: str = \"\"):\n",
    "        super(QuafuQuanv2d, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "\n",
    "        self.qkernel = qkernel or QuafuQKernel(in_channels=in_channels, kernel_size=kernel_size,\n",
    "                                               num_qlayers=num_qlayers, qdevice=qdevice, api_token=api_token)\n",
    "        self.qlayer = QuafuTorchLayer(self.qkernel, self.qkernel.weight_shapes)\n",
    "\n",
    "        # Use 1x1 classical convolution to match the desired output channels\n",
    "        self.classical_conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Verified params\n",
    "        assert len(x.shape) == 4, \"Input tensor must have 4 dimensions: (batch_size, channels, height, width)\"\n",
    "        assert x.dtype == torch.float32, \"Input tensor must have dtype torch.float32\"\n",
    "        assert x.shape[1] == self.in_channels, f\"Input tensor must have {self.in_channels} input channels\"\n",
    "\n",
    "        # Apply quantum convolution\n",
    "        x = self.quantum_conv(x)\n",
    "\n",
    "        # Apply 1x1 classical convolution to match the desired output channels\n",
    "        x = self.classical_conv(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def quantum_conv(self, x):\n",
    "        bs, _, h, w = x.shape\n",
    "\n",
    "        # Apply padding to the input tensor\n",
    "        if self.padding != 0:\n",
    "            x = F.pad(x, (self.padding,) * 4, mode=\"constant\", value=0)\n",
    "            h += 2 * self.padding\n",
    "            w += 2 * self.padding\n",
    "\n",
    "        # Unfold the input tensor to extract overlapping patches\n",
    "        patches = F.unfold(x, kernel_size=self.kernel_size, stride=self.stride, padding=self.padding)\n",
    "        patches = patches.permute(0, 2, 1)  # Reshape to (bs, num_patches, in_channels * kernel_size^2)\n",
    "\n",
    "        # Apply quantum kernel to x\n",
    "        out = []\n",
    "        # Apply quantum layer to each batch\n",
    "        for i in range(bs):\n",
    "            batch_out = [self.qlayer(patch) for patch in patches[i]]\n",
    "            out.append(torch.stack(batch_out))\n",
    "        out = torch.stack(out)\n",
    "\n",
    "        # Fold the output tensor\n",
    "        out = out.permute(0, 2, 1)\n",
    "        out = F.fold(out, output_size=(h, w), kernel_size=self.kernel_size, stride=self.stride, padding=self.padding)\n",
    "\n",
    "        # Normalization\n",
    "        ones = torch.ones_like(x)\n",
    "        unfolded_ones = F.unfold(ones, kernel_size=self.kernel_size, stride=self.stride, padding=self.padding)\n",
    "        folded_ones = F.fold(unfolded_ones, output_size=(h, w), kernel_size=self.kernel_size, stride=self.stride,\n",
    "                             padding=self.padding)\n",
    "        out = out / folded_ones\n",
    "\n",
    "        return out\n"
   ],
   "id": "191e49de7701f7a6",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 3. Models",
   "id": "8b569c0f2d6f5de5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 3.1 Benchmark Models",
   "id": "43e282cae920147e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class ClassicNet(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(ClassicNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=4, kernel_size=1, stride=1, padding=0)\n",
    "        self.conv2 = nn.Conv2d(in_channels=4, out_channels=8, kernel_size=2, stride=2, padding=0)\n",
    "        self.fc1 = nn.Linear(8 * 7 * 7, 128)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        bs = x.shape[0]\n",
    "        # Convolution\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        # Linear\n",
    "        x = x.view(bs, -1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ],
   "id": "78f5034eed2c1b62",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class HybridNet(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes=10, stride=2, **kwargs):\n",
    "        super(HybridNet, self).__init__()\n",
    "        self.quanv = Quanv2d(in_channels=1, out_channels=4, kernel_size=2, stride=stride, padding=0, **kwargs)\n",
    "        self.conv = nn.Conv2d(in_channels=4, out_channels=8, kernel_size=2, stride=2, padding=0)\n",
    "        self.fc1 = nn.Linear(8 * 7 * 7, 128)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        bs = x.shape[0]\n",
    "        # Quantum Convolution\n",
    "        x = self.quanv(x)\n",
    "        x = self.conv(x)\n",
    "        x = F.relu(x)\n",
    "        # Linear\n",
    "        x = x.view(bs, -1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x\n"
   ],
   "id": "47a01f5d23362c3c",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class QuafuHybridNet(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes=10, stride=2, **kwargs):\n",
    "        super(QuafuHybridNet, self).__init__()\n",
    "        self.quanv = QuafuQuanv2d(in_channels=1, out_channels=4, kernel_size=2, stride=stride, padding=0, **kwargs)\n",
    "        self.conv = nn.Conv2d(in_channels=4, out_channels=8, kernel_size=2, stride=2, padding=0)\n",
    "        self.fc1 = nn.Linear(8 * 7 * 7, 128)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        bs = x.shape[0]\n",
    "        # Quantum Convolution\n",
    "        x = self.quanv(x)\n",
    "        x = self.conv(x)\n",
    "        x = F.relu(x)\n",
    "        # Linear\n",
    "        x = x.view(bs, -1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x\n"
   ],
   "id": "32df954bef7811ff",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 3.2 Hybrid Models",
   "id": "ada00f5422db513e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3.2.1 VGG",
   "id": "f9bedd47c8eea6d0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class SimpleVGG(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(SimpleVGG, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            # Layer 1\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            # Layer 2\n",
    "            nn.Conv2d(32, 3, kernel_size=1, stride=1, padding=0),\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            # Layer 3\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((7, 7))\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(128 * 7 * 7, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n"
   ],
   "id": "1003157d7c8de6c9",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class HybridVGG(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes=10, **kwargs):\n",
    "        super(HybridVGG, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            # Layer 1\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            # Layer 2\n",
    "            # 先降维后升维\n",
    "            nn.Conv2d(32, 3, kernel_size=1, stride=1, padding=0),\n",
    "            Quanv2d(3, 64, kernel_size=2, stride=2, padding=0, **kwargs),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            # Layer 3\n",
    "            # N x 64 x 16 x 16\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((7, 7))\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(128 * 7 * 7, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n"
   ],
   "id": "b5edb386b796bf0",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# TODO: Hybrid Quafu",
   "id": "b2c651192c9e1514",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3.2.2 GoogLeNet",
   "id": "892fe27c810815d5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class SimpleGoogLeNet(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            num_classes: int = 10,\n",
    "            aux_logits: bool = True,\n",
    "            dropout: float = 0.4,\n",
    "            dropout_aux: float = 0.5,\n",
    "    ):\n",
    "        super(SimpleGoogLeNet, self).__init__()\n",
    "\n",
    "        self.aux_logits = aux_logits\n",
    "\n",
    "        self.conv1 = BasicConv2d(3, 16, kernel_size=3, stride=2, padding=1)\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True)\n",
    "        self.conv2 = BasicConv2d(16, 16, kernel_size=1)\n",
    "        self.conv3 = BasicConv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True)\n",
    "\n",
    "        self.inception3a = SimpleInception(32, 24, 12, 24, 4, 8, 12)\n",
    "        self.inception3b = SimpleInception(68, 48, 24, 32, 8, 12, 24)\n",
    "        self.maxpool3 = nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True)\n",
    "\n",
    "        self.inception4a = SimpleInception(116, 64, 12, 36, 4, 12, 36)\n",
    "        self.inception4b = SimpleInception(148, 48, 24, 48, 12, 24, 36)\n",
    "        self.maxpool4 = nn.MaxPool2d(kernel_size=2, stride=2, ceil_mode=True)\n",
    "\n",
    "        self.inception5a = SimpleInception(156, 72, 36, 64, 4, 12, 48)\n",
    "        self.inception5b = SimpleInception(196, 72, 36, 64, 24, 32, 48)\n",
    "\n",
    "        if self.aux_logits:\n",
    "            self.aux = SimpleInceptionAux(156, num_classes, dropout=dropout_aux)\n",
    "        else:\n",
    "            self.aux = None\n",
    "\n",
    "        self.avgpool = nn.AvgPool2d((1, 1))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(864, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # N x 3 x 64 x 64\n",
    "        x = self.conv1(x)\n",
    "        # N x 16 x 32 x 32\n",
    "        x = self.maxpool1(x)\n",
    "        # N x 16 x 16 x 16\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        # N x 32 x 16 x 16\n",
    "        x = self.maxpool2(x)\n",
    "\n",
    "        # N x 32 x 8 x 8\n",
    "        x = self.inception3a(x)\n",
    "        # N x 68 x 8 x 8\n",
    "        x = self.inception3b(x)\n",
    "        # N x 116 x 8 x 8\n",
    "        x = self.maxpool3(x)\n",
    "\n",
    "        # N x 116 x 4 x 4\n",
    "        x = self.inception4a(x)\n",
    "        # N x 148 x 4 x 4\n",
    "        x = self.inception4b(x)\n",
    "        # N x 156 x 2 x 2\n",
    "        aux: Optional[Tensor] = None\n",
    "        if self.training and self.aux:\n",
    "            aux = self.aux(x)\n",
    "        x = self.maxpool4(x)\n",
    "\n",
    "        # N x 156 x 2 x 2\n",
    "        x = self.inception5a(x)\n",
    "        # N x 196 x 2 x 2\n",
    "        x = self.inception5b(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        # N x 216 x 2 x 2\n",
    "        x = torch.flatten(x, 1)\n",
    "        # N x 864\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        if self.training and self.aux_logits:\n",
    "            return x, aux\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class SimpleInception(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_channels: int,\n",
    "            ch1x1: int,\n",
    "            ch3x3red: int,\n",
    "            ch3x3: int,\n",
    "            ch5x5red: int,\n",
    "            ch5x5: int,\n",
    "            pool_proj: int):\n",
    "        super(SimpleInception, self).__init__()\n",
    "\n",
    "        self.branch1 = BasicConv2d(in_channels, ch1x1, kernel_size=1)\n",
    "\n",
    "        self.branch2 = nn.Sequential(\n",
    "            BasicConv2d(in_channels, ch3x3red, kernel_size=1),\n",
    "            BasicConv2d(ch3x3red, ch3x3, kernel_size=3, padding=1)\n",
    "        )\n",
    "\n",
    "        self.branch3 = nn.Sequential(\n",
    "            BasicConv2d(in_channels, ch5x5red, kernel_size=1),\n",
    "            BasicConv2d(ch5x5red, ch5x5, kernel_size=3, padding=1)\n",
    "        )\n",
    "\n",
    "        self.branch4 = nn.Sequential(\n",
    "            nn.MaxPool2d(kernel_size=3, stride=1, padding=1),\n",
    "            BasicConv2d(in_channels, pool_proj, kernel_size=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        branch1 = self.branch1(x)\n",
    "        branch2 = self.branch2(x)\n",
    "        branch3 = self.branch3(x)\n",
    "        branch4 = self.branch4(x)\n",
    "\n",
    "        outputs = [branch1, branch2, branch3, branch4]\n",
    "        return torch.cat(outputs, 1)\n",
    "\n",
    "\n",
    "class SimpleInceptionAux(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_channels: int,\n",
    "            num_classes: int,\n",
    "            dropout: float = 0.5,\n",
    "    ):\n",
    "        super(SimpleInceptionAux, self).__init__()\n",
    "        self.averagePool = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        self.conv = BasicConv2d(in_channels, 128, kernel_size=1)\n",
    "        self.fc1 = nn.Linear(512, 128)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x):\n",
    "        # aux1: N x 156 x 4 x 4\n",
    "        x = self.averagePool(x)\n",
    "        # aux1: N x 156 x 2 x 2\n",
    "        x = self.conv(x)\n",
    "        # N x 128 x 2 x 2\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        # N x 512\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x, inplace=True)\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        # N x 128\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class BasicConv2d(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels: int, out_channels: int, **kwargs: Any) -> None:\n",
    "        super(BasicConv2d, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, bias=False, **kwargs)\n",
    "        self.bn = nn.BatchNorm2d(out_channels, eps=0.001)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n"
   ],
   "id": "8420f5f5dba92df1",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class HybridGoogLeNet(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            num_classes: int = 10,\n",
    "            aux_logits: bool = True,\n",
    "            dropout: float = 0.4,\n",
    "            dropout_aux: float = 0.5,\n",
    "            **kwargs: Any,\n",
    "    ):\n",
    "        super(HybridGoogLeNet, self).__init__()\n",
    "\n",
    "        self.aux_logits = aux_logits\n",
    "\n",
    "        self.conv1 = BasicConv2d(3, 16, kernel_size=3, stride=2, padding=1)\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True)\n",
    "        self.conv2 = BasicConv2d(16, 16, kernel_size=1)\n",
    "        self.conv3 = BasicConv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True)\n",
    "\n",
    "        self.inception3a = HybridInception(32, 24, 12, 24, 4, 8, 12, **kwargs)\n",
    "        self.inception3b = SimpleInception(68, 48, 24, 32, 8, 12, 24)\n",
    "        self.maxpool3 = nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True)\n",
    "\n",
    "        self.inception4a = HybridInception(116, 64, 12, 36, 4, 12, 36, **kwargs)\n",
    "        self.inception4b = SimpleInception(148, 48, 24, 48, 12, 24, 36)\n",
    "        self.maxpool4 = nn.MaxPool2d(kernel_size=2, stride=2, ceil_mode=True)\n",
    "\n",
    "        self.inception5a = HybridInception(156, 72, 36, 64, 4, 12, 48, **kwargs)\n",
    "        self.inception5b = SimpleInception(196, 72, 36, 64, 24, 32, 48)\n",
    "\n",
    "        if self.aux_logits:\n",
    "            self.aux = SimpleInceptionAux(156, num_classes, dropout=dropout_aux)\n",
    "        else:\n",
    "            self.aux = None\n",
    "\n",
    "        self.avgpool = nn.AvgPool2d((1, 1))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(864, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # N x 3 x 64 x 64\n",
    "        x = self.conv1(x)\n",
    "        # N x 16 x 32 x 32\n",
    "        x = self.maxpool1(x)\n",
    "        # N x 16 x 16 x 16\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        # N x 32 x 16 x 16\n",
    "        x = self.maxpool2(x)\n",
    "\n",
    "        # N x 32 x 8 x 8\n",
    "        x = self.inception3a(x)\n",
    "        # N x 68 x 8 x 8\n",
    "        x = self.inception3b(x)\n",
    "        # N x 116 x 8 x 8\n",
    "        x = self.maxpool3(x)\n",
    "\n",
    "        # N x 116 x 4 x 4\n",
    "        x = self.inception4a(x)\n",
    "        # N x 148 x 4 x 4\n",
    "        x = self.inception4b(x)\n",
    "        # N x 156 x 2 x 2\n",
    "        aux: Optional[Tensor] = None\n",
    "        if self.training and self.aux:\n",
    "            aux = self.aux(x)\n",
    "        x = self.maxpool4(x)\n",
    "\n",
    "        # N x 156 x 2 x 2\n",
    "        x = self.inception5a(x)\n",
    "        # N x 196 x 2 x 2\n",
    "        x = self.inception5b(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        # N x 216 x 2 x 2\n",
    "        x = torch.flatten(x, 1)\n",
    "        # N x 864\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        if self.training and self.aux_logits:\n",
    "            return x, aux\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class HybridInception(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_channels: int,\n",
    "            ch1x1: int,\n",
    "            ch3x3red: int,\n",
    "            ch3x3: int,\n",
    "            ch5x5red: int,\n",
    "            ch5x5: int,\n",
    "            pool_proj: int,\n",
    "            **kwargs: Any,\n",
    "    ):\n",
    "        super(HybridInception, self).__init__()\n",
    "\n",
    "        self.branch1 = BasicConv2d(in_channels, ch1x1, kernel_size=1)\n",
    "\n",
    "        self.branch2 = nn.Sequential(\n",
    "            BasicConv2d(in_channels, ch3x3red, kernel_size=1),\n",
    "            BasicConv2d(ch3x3red, ch3x3, kernel_size=3, padding=1)\n",
    "        )\n",
    "\n",
    "        self.branch3 = nn.Sequential(\n",
    "            BasicConv2d(in_channels, ch5x5red, kernel_size=1),\n",
    "            HybridConv2d(ch5x5red, ch5x5, kernel_size=2, stride=2, padding=0, **kwargs)\n",
    "        )\n",
    "\n",
    "        self.branch4 = nn.Sequential(\n",
    "            nn.MaxPool2d(kernel_size=3, stride=1, padding=1),\n",
    "            BasicConv2d(in_channels, pool_proj, kernel_size=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        branch1 = self.branch1(x)\n",
    "        branch2 = self.branch2(x)\n",
    "        branch3 = self.branch3(x)\n",
    "        branch4 = self.branch4(x)\n",
    "\n",
    "        outputs = [branch1, branch2, branch3, branch4]\n",
    "        return torch.cat(outputs, 1)\n",
    "\n",
    "\n",
    "class HybridConv2d(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels: int, out_channels: int, **kwargs: Any) -> None:\n",
    "        super(HybridConv2d, self).__init__()\n",
    "        self.quanv = Quanv2d(in_channels, out_channels, **kwargs)\n",
    "        self.bn = nn.BatchNorm2d(out_channels, eps=0.001)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.quanv(x)\n",
    "        x = self.bn(x)\n",
    "        return x\n"
   ],
   "id": "616320c973de3833",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# TODO: Hybrid Quafu",
   "id": "3d9722c7e75269d4",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3.2.3 ResNet",
   "id": "6a486c92d798f062"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_channels: int,\n",
    "            out_channels: int,\n",
    "            stride: int = 1,\n",
    "            downsample: Optional[nn.Module] = None,\n",
    "    ):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        # Layer 1\n",
    "        self.conv1 = nn.Conv2d(in_channels=in_channels, out_channels=out_channels,\n",
    "                               kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        # Layer 2\n",
    "        self.conv2 = nn.Conv2d(in_channels=out_channels, out_channels=4,\n",
    "                               kernel_size=1, stride=1, padding=0, bias=False)\n",
    "        self.conv3 = nn.Conv2d(in_channels=4, out_channels=out_channels,\n",
    "                               kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        # Layer 3\n",
    "        self.conv4 = nn.Conv2d(in_channels=out_channels, out_channels=out_channels,\n",
    "                               kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.conv3(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv4(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class SimpleResNet(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            block: Type[Union[BasicBlock]],\n",
    "            num_blocks: List[int],\n",
    "            num_classes: int = 10,\n",
    "    ):\n",
    "        super(SimpleResNet, self).__init__()\n",
    "\n",
    "        self.in_channels = 16\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=self.in_channels,\n",
    "                               kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(self.in_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        self.layer1 = self._make_layer(block, 16, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 32, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 64, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 128, num_blocks[3], stride=2)\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(128, num_classes)\n",
    "\n",
    "    def _make_layer(\n",
    "            self,\n",
    "            block: Type[Union[BasicBlock]],\n",
    "            channels: int,\n",
    "            block_num: int,\n",
    "            stride: int = 1,\n",
    "    ) -> nn.Sequential:\n",
    "        downsample = None\n",
    "        if stride != 1 or self.in_channels != channels * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.in_channels, channels * block.expansion, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(channels * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = [\n",
    "            block(self.in_channels, channels, stride=stride, downsample=downsample),\n",
    "        ]\n",
    "        self.in_channels = channels * block.expansion\n",
    "\n",
    "        for _ in range(1, block_num):\n",
    "            layers.append(block(self.in_channels, channels))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def simple_resnet18(num_classes=10):\n",
    "    return SimpleResNet(BasicBlock, [2, 2, 2, 2], num_classes)\n",
    "\n",
    "\n",
    "def simple_resnet34(num_classes=10):\n",
    "    return SimpleResNet(BasicBlock, [3, 4, 6, 3], num_classes)\n"
   ],
   "id": "815049e13e9feeeb",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class HybridBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_channels: int,\n",
    "            out_channels: int,\n",
    "            stride: int = 1,\n",
    "            downsample: Optional[nn.Module] = None,\n",
    "            **kwargs: Any,\n",
    "    ):\n",
    "        super(HybridBlock, self).__init__()\n",
    "        # Layer 1\n",
    "        self.conv1 = nn.Conv2d(in_channels=in_channels, out_channels=out_channels,\n",
    "                               kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        # Layer 2\n",
    "        self.conv2 = nn.Conv2d(in_channels=out_channels, out_channels=4,\n",
    "                               kernel_size=1, stride=1, padding=0, bias=False)\n",
    "        self.quanv = Quanv2d(in_channels=4, out_channels=out_channels, kernel_size=2, stride=2, padding=0, **kwargs)\n",
    "        # Layer 3\n",
    "        self.conv3 = nn.Conv2d(in_channels=out_channels, out_channels=out_channels,\n",
    "                               kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.quanv(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class HybridResNet(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            block: Type[Union[BasicBlock]],\n",
    "            num_blocks: List[int],\n",
    "            num_classes: int = 10,\n",
    "            **kwargs: Any,\n",
    "    ):\n",
    "        super(HybridResNet, self).__init__()\n",
    "\n",
    "        self.in_channels = 16\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=self.in_channels,\n",
    "                               kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(self.in_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        self.layer1 = self._make_layer(block, 16, num_blocks[0], stride=1, use_quanv2d=False, **kwargs)\n",
    "        self.layer2 = self._make_layer(block, 32, num_blocks[1], stride=2, use_quanv2d=False, **kwargs)\n",
    "        self.layer3 = self._make_layer(block, 64, num_blocks[2], stride=2, use_quanv2d=True, **kwargs)\n",
    "        self.layer4 = self._make_layer(block, 128, num_blocks[3], stride=2, use_quanv2d=True, **kwargs)\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(128, num_classes)\n",
    "\n",
    "    def _make_layer(\n",
    "            self,\n",
    "            block: Type[Union[BasicBlock]],\n",
    "            channels: int,\n",
    "            block_num: int,\n",
    "            stride: int = 1,\n",
    "            use_quanv2d: bool = False,\n",
    "            **kwargs: Any,\n",
    "    ) -> nn.Sequential:\n",
    "        downsample = None\n",
    "        if stride != 1 or self.in_channels != channels * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.in_channels, channels * block.expansion, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(channels * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        if use_quanv2d:\n",
    "            layers.append(HybridBlock(self.in_channels, channels, stride=stride, downsample=downsample, **kwargs))\n",
    "        else:\n",
    "            layers.append(block(self.in_channels, channels, stride=stride, downsample=downsample))\n",
    "        self.in_channels = channels * block.expansion\n",
    "\n",
    "        for _ in range(1, block_num):\n",
    "            layers.append(block(self.in_channels, channels))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def hybrid_resnet18(num_classes=10, **kwargs):\n",
    "    return HybridResNet(BasicBlock, [2, 2, 2, 2], num_classes, **kwargs)\n",
    "\n",
    "\n",
    "def hybrid_resnet34(num_classes=10, **kwargs):\n",
    "    return HybridResNet(BasicBlock, [3, 4, 6, 3], num_classes, **kwargs)\n"
   ],
   "id": "58c82af9f533992e",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# TODO: Hybrid Quafu",
   "id": "4307ead2f68798ca",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "ALL_MODELS = {\n",
    "    'ClassicNet': ClassicNet,\n",
    "    'HybridNet': HybridNet,\n",
    "    'SimpleVGG': SimpleVGG,\n",
    "    'HybridVGG': HybridVGG,\n",
    "    'SimpleGoogLeNet': SimpleGoogLeNet,\n",
    "    'HybridGoogLeNet': HybridGoogLeNet,\n",
    "    'SimpleResNet': SimpleResNet,\n",
    "    'HybridResNet': HybridResNet,\n",
    "}"
   ],
   "id": "512428639281c8c7",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 4. Dataset",
   "id": "aa87a9564565b44e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class GarbageDataset(Dataset):\n",
    "\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "        self.classes = [\n",
    "            'battery', 'biological', 'brown-glass', 'cardboard', 'clothes',\n",
    "            'metal', 'paper', 'plastic', 'shoes', 'trash',\n",
    "        ]\n",
    "\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "        for index, class_name in enumerate(self.classes):\n",
    "            class_dir = os.path.join(self.root_dir, class_name)\n",
    "            class_dir = str(class_dir)  # This line is not required and is only used to eliminate PyCharm's warnings.\n",
    "            for image in os.listdir(class_dir):\n",
    "                image_path = os.path.join(class_dir, image)\n",
    "                self.images.append(image_path)\n",
    "                self.labels.append(index)\n",
    "\n",
    "    def set_transform(self, transform):\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_path = self.images[index]\n",
    "        label = self.labels[index]\n",
    "\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n"
   ],
   "id": "ced9da243770655e",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 5. Tools",
   "id": "1d96bf823f3731cf"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 5.1 Dataset Tools",
   "id": "6068d10ffcfa9da8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def extract_images_from_datasets(dataset, num_pics, out_dir='./pics/ext_pics/'):\n",
    "    \"\"\"\n",
    "    Save a specified number of randomly selected images from a dataset to an output directory.\n",
    "    \"\"\"\n",
    "    labels = dataset.classes\n",
    "    random_indexes = random.sample(range(len(dataset)), num_pics)\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    # Save data\n",
    "    for idx in random_indexes:\n",
    "        image, label = dataset[idx]\n",
    "        class_name = labels[label]\n",
    "        image_path = os.path.join(out_dir, class_name, f'{idx}.jpg')\n",
    "        os.makedirs(os.path.dirname(image_path), exist_ok=True)\n",
    "\n",
    "        try:\n",
    "            image.save(image_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to save image {idx}: {str(e)}\")\n",
    "\n",
    "\n",
    "def save_class_indices(dataset, json_file_path):\n",
    "    \"\"\"\n",
    "    Save the class information and their corresponding indices from the dataset to a JSON file.\n",
    "    \"\"\"\n",
    "    labels = dataset.classes\n",
    "    label_dict = {i: label for i, label in enumerate(labels)}\n",
    "\n",
    "    with open(json_file_path, \"w\") as json_file:\n",
    "        json.dump(label_dict, json_file, indent=4)\n",
    "\n",
    "    print(f\"Class labels and their indices successfully saved to {json_file_path}.\")\n"
   ],
   "id": "ecf519cfd597c2fd",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 5.2 Quantum Transforms",
   "id": "fb630c3fe33ffe09"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class ToTensor4Quantum:\n",
    "    \"\"\"\n",
    "    Transform class to convert a PIL Image or ndarray to tensor and scale the values suitable for quantum computing.\n",
    "\n",
    "    This class normalizes the input image and converts it to a tensor representation.\n",
    "    The normalization process includes scaling pixel values in the image to the range (0, π).\n",
    "\n",
    "    Usage:\n",
    "        transform = ToTensor4Quantum()\n",
    "        tensor_image = transform(image)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "\n",
    "    @wraps(torch.tensor)\n",
    "    def __call__(self, pic):\n",
    "        return np.pi * TF.to_tensor(pic)\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"{self.__class__.__name__}()\"\n"
   ],
   "id": "eb328dbd8a67f8d8",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 5.3 Visualization Tools",
   "id": "66c1a694cf0cd028"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Constants\n",
    "TITLE_FONTSIZE = 16\n",
    "LABEL_FONTSIZE = 14\n",
    "TICK_FONTSIZE = 12\n",
    "LEGEND_FONTSIZE = 12\n",
    "FIGSIZE_WIDTH = 12\n",
    "FIGSIZE_HEIGHT_PER_ROW = 3\n",
    "PROBABILITY_FIGSIZE_WIDTH = 18\n",
    "PROBABILITY_FIGSIZE_HEIGHT_PER_ROW = 6\n",
    "\n",
    "\n",
    "def save_confusion_matrix(confusion_matrix_data, output_path):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(confusion_matrix_data, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.xlabel('Predicted Labels')\n",
    "    plt.ylabel('True Labels')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_path)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def set_plot_style(ax, title, xlabel, ylabel):\n",
    "    \"\"\"\n",
    "    Set the style of a subplot.\n",
    "    \"\"\"\n",
    "    ax.set_title(title, fontsize=TITLE_FONTSIZE)\n",
    "    ax.set_xlabel(xlabel, fontsize=LABEL_FONTSIZE)\n",
    "    ax.set_ylabel(ylabel, fontsize=LABEL_FONTSIZE)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=TICK_FONTSIZE)\n",
    "\n",
    "\n",
    "def plot_evaluation_metrics(models_linestyles, data_types, data_dir='./output/', show=True, save=False,\n",
    "                            save_path='./output/evaluation_metrics_chart.png', names=None):\n",
    "    \"\"\"\n",
    "    Plot models' evaluation metrics as line charts.\n",
    "    \"\"\"\n",
    "    # Set subplot layout\n",
    "    num_data_types = len(data_types)\n",
    "    num_columns = 2\n",
    "    num_rows = (num_data_types + num_columns - 1) // num_columns\n",
    "    fig, axes = plt.subplots(num_rows, num_columns, figsize=(FIGSIZE_WIDTH, FIGSIZE_HEIGHT_PER_ROW * num_rows))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    # Plot subplots\n",
    "    for idx, data_type in enumerate(data_types):\n",
    "        ax = axes[idx]\n",
    "        title = data_type.replace('_', ' ').title()\n",
    "        ylabel = data_type.split('_')[1].title()\n",
    "        set_plot_style(ax, title, 'Epoch', ylabel)\n",
    "\n",
    "        for model_name, linestyle in models_linestyles.items():\n",
    "            data = load_evaluation_metrics(model_name, data_type, data_dir)\n",
    "            # ax.plot(data, label=model_name, linestyle=linestyle)\n",
    "            label = names.get(model_name, model_name) if names else model_name\n",
    "            ax.plot(data, label=label, linestyle=linestyle)\n",
    "            ax.legend(fontsize=LEGEND_FONTSIZE)\n",
    "\n",
    "    for i in range(num_data_types, num_columns * num_rows):\n",
    "        fig.delaxes(axes[i])\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if save:\n",
    "        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "        plt.savefig(save_path)\n",
    "\n",
    "    if show:\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def plot_probabilities(results, labels, show=True, save=False, save_path='./probabilities.png', colors=None):\n",
    "    \"\"\"\n",
    "    Plot probability distribution charts for multiple model classification results.\n",
    "    \"\"\"\n",
    "    if not results:\n",
    "        print(\"No results to plot.\")\n",
    "        return\n",
    "\n",
    "    # Default color list\n",
    "    if colors is None:\n",
    "        colors = ['blue', 'green', 'red', 'purple', 'orange', 'cyan']\n",
    "\n",
    "    # Set subplot layout\n",
    "    num_models = len(results)\n",
    "    num_columns = 3\n",
    "    num_rows = (num_models + num_columns - 1) // num_columns\n",
    "    fig, axes = plt.subplots(num_rows, num_columns,\n",
    "                             figsize=(PROBABILITY_FIGSIZE_WIDTH, PROBABILITY_FIGSIZE_HEIGHT_PER_ROW * num_rows))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    # Plot probability distribution charts\n",
    "    for idx, (model_name, result) in enumerate(results.items()):\n",
    "        ax = axes[idx]  # 获取当前子图\n",
    "\n",
    "        probabilities = result['probabilities'][0]\n",
    "        ax.bar(labels, probabilities, color=colors[idx % len(colors)])\n",
    "        set_plot_style(ax, f'{model_name} Prediction', '', '')\n",
    "        ax.set_xticks(labels)\n",
    "        ax.set_xticklabels(labels, rotation=45, ha='right')\n",
    "        ax.set_ylim(0, 0.3)\n",
    "\n",
    "    # Clear unused subplots\n",
    "    for i in range(num_models, num_columns * num_rows):\n",
    "        fig.delaxes(axes[i])\n",
    "\n",
    "    # Adjust spacing between subplots\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save probability distribution charts\n",
    "    if save:\n",
    "        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "        plt.savefig(save_path)\n",
    "\n",
    "    # Show probability distribution charts\n",
    "    if show:\n",
    "        plt.show()\n"
   ],
   "id": "7ed95d9599cc6409",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 5.4 Model Tools",
   "id": "8672207c032e0c32"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def set_random_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "def get_models(dataset_name, qdevice, qdevice_kwargs, diff_method):\n",
    "    if dataset_name == 'FashionMNIST':\n",
    "        return {\n",
    "            'ClassicNet': ClassicNet(num_classes=10),\n",
    "            'HybridNet': HybridNet(num_classes=10, qkernel=None, num_qlayers=1,\n",
    "                                   qdevice=qdevice, qdevice_kwargs=qdevice_kwargs, diff_method=diff_method),\n",
    "            'HybridNetDeeper': HybridNet(num_classes=10, qkernel=None, num_qlayers=2,\n",
    "                                         qdevice=qdevice, qdevice_kwargs=qdevice_kwargs, diff_method=diff_method),\n",
    "            'HybridNetStrideOne': HybridNet(num_classes=10, qkernel=None, num_qlayers=2, stride=1,\n",
    "                                            qdevice=qdevice, qdevice_kwargs=qdevice_kwargs, diff_method=diff_method),\n",
    "            # TODO: About Barren Plateau\n",
    "            'HybridNetDeeper2': HybridNet(num_classes=10, qkernel=None, num_qlayers=3,\n",
    "                                          qdevice=qdevice, qdevice_kwargs=qdevice_kwargs, diff_method=diff_method),\n",
    "        }\n",
    "    elif dataset_name == 'GarbageDataset':\n",
    "        return {\n",
    "            'SimpleVGG': SimpleVGG(num_classes=10),\n",
    "            'HybridVGG': HybridVGG(num_classes=10, qkernel=None, num_qlayers=2,\n",
    "                                   qdevice=qdevice, qdevice_kwargs=qdevice_kwargs, diff_method=diff_method),\n",
    "            'SimpleGoogLeNet': SimpleGoogLeNet(num_classes=10),\n",
    "            'HybridGoogLeNet': HybridGoogLeNet(num_classes=10, qkernel=None, num_qlayers=2,\n",
    "                                               qdevice=qdevice, qdevice_kwargs=qdevice_kwargs, diff_method=diff_method),\n",
    "            'SimpleResNet': simple_resnet18(num_classes=10),\n",
    "            'HybridResNet': hybrid_resnet18(num_classes=10, qkernel=None, num_qlayers=2,\n",
    "                                            qdevice=qdevice, qdevice_kwargs=qdevice_kwargs, diff_method=diff_method),\n",
    "        }\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported dataset: {dataset_name}\")\n",
    "\n",
    "\n",
    "def get_transforms(model_name, dataset_name):\n",
    "    model_type = 'Hybrid' if 'Hybrid' in model_name else 'Classic'\n",
    "    if dataset_name == 'FashionMNIST' and model_type == 'Classic':\n",
    "        train_transform = test_transform = torchvision.transforms.Compose([\n",
    "            torchvision.transforms.Resize((14, 14)),\n",
    "            torchvision.transforms.ToTensor(),\n",
    "        ])\n",
    "    elif dataset_name == 'FashionMNIST' and model_type == 'Hybrid':\n",
    "        train_transform = test_transform = torchvision.transforms.Compose([\n",
    "            torchvision.transforms.Resize((14, 14)),\n",
    "            ToTensor4Quantum(),\n",
    "        ])\n",
    "    elif dataset_name == 'GarbageDataset' and model_type == 'Classic':\n",
    "        train_transform = torchvision.transforms.Compose([\n",
    "            torchvision.transforms.Resize(72),\n",
    "            torchvision.transforms.RandomCrop(64),\n",
    "            torchvision.transforms.RandomHorizontalFlip(),\n",
    "            torchvision.transforms.RandomRotation(degrees=15),\n",
    "            torchvision.transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1),\n",
    "            torchvision.transforms.ToTensor(),\n",
    "        ])\n",
    "        test_transform = torchvision.transforms.Compose([\n",
    "            torchvision.transforms.Resize(72),\n",
    "            torchvision.transforms.CenterCrop(64),\n",
    "            torchvision.transforms.ToTensor(),\n",
    "        ])\n",
    "    elif dataset_name == 'GarbageDataset' and model_type == 'Hybrid':\n",
    "        train_transform = torchvision.transforms.Compose([\n",
    "            torchvision.transforms.Resize(72),\n",
    "            torchvision.transforms.RandomCrop(64),\n",
    "            torchvision.transforms.RandomHorizontalFlip(),\n",
    "            torchvision.transforms.RandomRotation(degrees=15),\n",
    "            torchvision.transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1),\n",
    "            ToTensor4Quantum(),\n",
    "        ])\n",
    "        test_transform = torchvision.transforms.Compose([\n",
    "            torchvision.transforms.Resize(72),\n",
    "            torchvision.transforms.CenterCrop(64),\n",
    "            ToTensor4Quantum(),\n",
    "        ])\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported dataset: {dataset_name}\")\n",
    "\n",
    "    return train_transform, test_transform\n",
    "\n",
    "\n",
    "def get_dataset(dataset_name, data_dir, train_transform, test_transform, batch_size):\n",
    "    if dataset_name == 'FashionMNIST':\n",
    "        train_set = torchvision.datasets.FashionMNIST(root=data_dir, train=True, transform=train_transform,\n",
    "                                                      download=True)\n",
    "        test_set = torchvision.datasets.FashionMNIST(root=data_dir, train=False, transform=test_transform,\n",
    "                                                     download=True)\n",
    "    elif dataset_name == 'GarbageDataset':\n",
    "        train_set = GarbageDataset(root_dir=os.path.join(data_dir, 'GarbageDataset', 'train'),\n",
    "                                   transform=train_transform)\n",
    "        test_set = GarbageDataset(root_dir=os.path.join(data_dir, 'GarbageDataset', 'test'),\n",
    "                                  transform=test_transform)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported dataset: {dataset_name}\")\n",
    "\n",
    "    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, test_loader\n",
    "\n",
    "\n",
    "def construct_file_path(model_name, data_type, data_dir='./output/'):\n",
    "    \"\"\"\n",
    "    Construct the file path based on model name and data type.\n",
    "    \"\"\"\n",
    "    return os.path.join(data_dir, model_name, f'{model_name}_{data_type}.npy')\n",
    "\n",
    "\n",
    "def load_model_with_weights(model_name, model_weights_path, num_classes, device, **kwargs):\n",
    "    \"\"\"\n",
    "    Load weights data into the specified model and return the model.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if model_name not in ALL_MODELS:\n",
    "            raise ValueError(\"无效的模型名称\")\n",
    "\n",
    "        model = ALL_MODELS[model_name](num_classes=num_classes, **kwargs)\n",
    "\n",
    "        try:\n",
    "            model.load_state_dict(torch.load(model_weights_path, map_location=device))\n",
    "        except FileNotFoundError:\n",
    "            logging.error(f\"Model weights file not found: {model_weights_path}\")\n",
    "            return None\n",
    "\n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "\n",
    "        return model\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error loading the model: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def save_evaluation_metrics(model_name, data, data_type, data_dir='./output/'):\n",
    "    \"\"\"\n",
    "    Save evaluation metrics to the specified path.\n",
    "    \"\"\"\n",
    "    data_file_path = construct_file_path(model_name, data_type, data_dir)\n",
    "    os.makedirs(os.path.dirname(data_file_path), exist_ok=True)\n",
    "    np.save(data_file_path, data)\n",
    "\n",
    "\n",
    "def load_evaluation_metrics(model_name, data_type, data_dir='./output/'):\n",
    "    \"\"\"\n",
    "    Load evaluation metrics data from a file.\n",
    "    \"\"\"\n",
    "    data_file_path = construct_file_path(model_name, data_type, data_dir)\n",
    "    if os.path.exists(data_file_path):\n",
    "        return np.load(data_file_path)\n",
    "    else:\n",
    "        logging.warning(f\"Evaluation metrics file not found: {data_file_path}\")\n",
    "        return None\n"
   ],
   "id": "ee614bf673101d14",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 6. Train",
   "id": "8ba55e6319b08b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def train(model_name, model, train_loader, test_loader, optimizer, scheduler, criterion, device, num_epochs, output_dir,\n",
    "          tensorboard_dir, aux_weight=0.4):\n",
    "    logger = logging.getLogger(__name__)\n",
    "    logger.info(f\"Start training for {num_epochs} epochs.\")\n",
    "\n",
    "    model_dir = os.path.join(output_dir, model_name)\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    model_weights_path = os.path.join(str(model_dir), f\"{model_name}_model.pth\")\n",
    "\n",
    "    writer = SummaryWriter(log_dir=tensorboard_dir)\n",
    "\n",
    "    # Move the model to the specified device\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Initialize lists to store statistics\n",
    "    best_test_acc = 0.0\n",
    "    train_loss_history = []\n",
    "    test_loss_history = []\n",
    "    train_acc_history = []\n",
    "    test_acc_history = []\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_acc = 0.0\n",
    "        train_samples = 0\n",
    "        train_bar = tqdm(train_loader, desc=f\"Epoch {epoch} - Training\")\n",
    "        for img, label in train_bar:\n",
    "            img, label = img.to(device), label.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            if 'GoogLeNet' in model_name:\n",
    "                output, aux_output = model(img)\n",
    "                loss1 = criterion(output, label)\n",
    "                loss2 = criterion(aux_output, label)\n",
    "                loss = loss1 + aux_weight * loss2\n",
    "            else:\n",
    "                output = model(img)\n",
    "                loss = criterion(output, label)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += abs(loss.item()) * img.size(0)\n",
    "            accuracy = torch.sum(torch.argmax(output, dim=1) == label).item()\n",
    "            train_acc += accuracy\n",
    "            train_samples += img.size(0)\n",
    "\n",
    "            train_bar.set_postfix(loss=loss.item(), accuracy=accuracy / img.size(0))\n",
    "\n",
    "        train_loss /= train_samples\n",
    "        train_acc /= train_samples\n",
    "        train_loss_history.append(train_loss)\n",
    "        train_acc_history.append(train_acc)\n",
    "\n",
    "        # Testing\n",
    "        model.eval()\n",
    "        test_loss = 0.0\n",
    "        test_acc = 0.0\n",
    "        test_samples = 0\n",
    "        with torch.no_grad():\n",
    "            test_bar = tqdm(test_loader, desc=f\"Epoch {epoch} - Testing\")\n",
    "            for img, label in test_bar:\n",
    "                img, label = img.to(device), label.to(device)\n",
    "                output = model(img)\n",
    "                loss = criterion(output, label)\n",
    "\n",
    "                test_loss += abs(loss.item()) * img.size(0)\n",
    "                accuracy = torch.sum(torch.argmax(output, dim=1) == label).item()\n",
    "                test_acc += accuracy\n",
    "                test_samples += img.size(0)\n",
    "\n",
    "                test_bar.set_postfix(loss=loss.item(), accuracy=accuracy / img.size(0))\n",
    "\n",
    "        test_loss /= test_samples\n",
    "        test_acc /= test_samples\n",
    "        test_loss_history.append(test_loss)\n",
    "        test_acc_history.append(test_acc)\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        writer.add_scalar(\"Loss/train\", train_loss, epoch)\n",
    "        writer.add_scalar(\"Accuracy/train\", train_acc, epoch)\n",
    "        writer.add_scalar(\"Loss/test\", test_loss, epoch)\n",
    "        writer.add_scalar(\"Accuracy/test\", test_acc, epoch)\n",
    "\n",
    "        if test_acc > best_test_acc:\n",
    "            best_test_acc = test_acc\n",
    "            torch.save(model.state_dict(), model_weights_path)\n",
    "\n",
    "        elapsed_time = time.time() - start_time\n",
    "        logger.info(f\"Epoch [{epoch}/{num_epochs}] - Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, \"\n",
    "                    f\"Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.4f}, Elapsed Time: {elapsed_time:.2f}s\")\n",
    "\n",
    "    writer.close()\n",
    "\n",
    "    # Save evaluation metrics\n",
    "    save_evaluation_metrics(model_name, train_loss_history, \"train_loss\", output_dir)\n",
    "    save_evaluation_metrics(model_name, train_acc_history, \"train_accuracy\", output_dir)\n",
    "    save_evaluation_metrics(model_name, test_loss_history, \"test_loss\", output_dir)\n",
    "    save_evaluation_metrics(model_name, test_acc_history, \"test_accuracy\", output_dir)\n",
    "    logger.info(f\"Training completed. Best test accuracy: {best_test_acc:.4f}\")\n"
   ],
   "id": "9a4be31d11dfb2db",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def main4train(args):\n",
    "    logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "    logger = logging.getLogger(__name__)\n",
    "\n",
    "    # Set Random Seed\n",
    "    set_random_seed(args['seed'])\n",
    "\n",
    "    # Set Device\n",
    "    if args['device'] is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    else:\n",
    "        device = torch.device(args['device'])\n",
    "    logger.info(f\"Using device: {device}\")\n",
    "\n",
    "    # Set Model\n",
    "    models = get_models(args['dataset'], args['qdevice'], args['qdevice_kwargs'], args['diff_method'])\n",
    "    model = models[args['model']]\n",
    "    model.to(device)\n",
    "\n",
    "    # Set Optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=args['lr'], weight_decay=args['weight_decay'])\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    criterion = criterion.to(device)\n",
    "\n",
    "    # Set Datasets\n",
    "    train_transform, test_transform = get_transforms(args['model'], args['dataset'])\n",
    "    train_loader, test_loader = get_dataset(args['dataset'], args['data_dir'], train_transform, test_transform,\n",
    "                                            args['batch_size'])\n",
    "\n",
    "    # Set Output Dir\n",
    "    os.makedirs(args['output_dir'], exist_ok=True)\n",
    "    os.makedirs(args['tensorboard_dir'], exist_ok=True)\n",
    "\n",
    "    # Train\n",
    "    train(args['model'], model, train_loader, test_loader, optimizer, scheduler, criterion, device, args['epochs'],\n",
    "          args['output_dir'], args['tensorboard_dir'])\n"
   ],
   "id": "ce539f863db4a3b4",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "TRAIN = True\n",
    "train_args = {\n",
    "    \"model\": \"ClassicNet\",\n",
    "    \"dataset\": \"FashionMNIST\",\n",
    "    \"data_dir\": \"../datasets\",\n",
    "    \"epochs\": 10,\n",
    "    \"batch_size\": 64,\n",
    "    \"lr\": 1e-3,\n",
    "    \"weight_decay\": 1e-4,\n",
    "    \"seed\": 42,\n",
    "    \"log_interval\": 10,\n",
    "    \"output_dir\": \"../output/Demo\",\n",
    "    \"tensorboard_dir\": \"../tensorboard\",\n",
    "    \"device\": 'cpu',\n",
    "    \"qdevice\": \"lightning.qubit\",\n",
    "    \"qdevice_kwargs\": None,\n",
    "    \"diff_method\": \"adjoint\"\n",
    "}\n",
    "if TRAIN:\n",
    "    main4train(train_args)"
   ],
   "id": "fbf84c7f771a269b",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 7. Test",
   "id": "c8e6dcad2bec2782"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def test(model_name, model, test_loader, criterion, device, output_dir):\n",
    "    logger = logging.getLogger(__name__)\n",
    "    logger.info(f\"Start testing.\")\n",
    "\n",
    "    # Move the model to the specified device\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Evaluate the model\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    test_acc = 0.0\n",
    "    test_samples = 0\n",
    "    test_labels = []\n",
    "    test_predictions = []\n",
    "    with torch.no_grad():\n",
    "        test_bar = tqdm(test_loader, desc=f\"Testing\")\n",
    "        for img, label in test_bar:\n",
    "            img, label = img.to(device), label.to(device)\n",
    "            output = model(img)\n",
    "            loss = criterion(output, label)\n",
    "\n",
    "            # test_loss += abs(loss.item()) * img.size(0)\n",
    "            test_loss += loss.item() * img.size(0)\n",
    "            accuracy = torch.sum(torch.argmax(output, dim=1) == label).item()\n",
    "            test_acc += accuracy\n",
    "            test_samples += img.size(0)\n",
    "\n",
    "            test_labels.extend(label.cpu().numpy())\n",
    "            test_predictions.extend(torch.argmax(output, dim=1).cpu().numpy())\n",
    "\n",
    "            test_bar.set_postfix(loss=loss.item(), accuracy=accuracy / img.size(0))\n",
    "\n",
    "    test_loss /= test_samples\n",
    "    test_acc /= test_samples\n",
    "    test_recall = recall_score(test_labels, test_predictions, average='macro')\n",
    "    test_f1 = f1_score(test_labels, test_predictions, average='macro')\n",
    "    confusion_matrix_data = confusion_matrix(test_labels, test_predictions)\n",
    "\n",
    "    # Save evaluation metrics\n",
    "    output_path = str(os.path.join(output_dir, model_name))\n",
    "    metrics = {\n",
    "        \"test_loss\": test_loss,\n",
    "        \"test_accuracy\": test_acc,\n",
    "        \"test_recall\": test_recall,\n",
    "        \"test_f1\": test_f1,\n",
    "    }\n",
    "    with open(os.path.join(output_path, f\"{model_name}_test_metrics.json\"), \"w\") as f:\n",
    "        json.dump(metrics, f, indent=4)\n",
    "\n",
    "    # Save Confusion Matrix\n",
    "    save_confusion_matrix(confusion_matrix_data, os.path.join(output_path, f\"{model_name}_confusion_matrix.png\"))\n",
    "\n",
    "    logger.info(f\"Testing completed. Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.4f}, \"\n",
    "                f\"Test Recall: {test_recall:.4f}, Test F1: {test_f1:.4f}\")\n"
   ],
   "id": "392f0eb0f1bb4a2b",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def main4test(args):\n",
    "    logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "    logger = logging.getLogger(__name__)\n",
    "\n",
    "    # Set Random Seed\n",
    "    set_random_seed(args['seed'])\n",
    "\n",
    "    # Set Device\n",
    "    if args['device'] is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    else:\n",
    "        device = torch.device(args['device'])\n",
    "    logger.info(f\"Using device: {device}\")\n",
    "\n",
    "    # Set Model\n",
    "    models = get_models(args['dataset'], args['qdevice'], args['qdevice_kwargs'], args['diff_method'])\n",
    "    model = models[args['model']]\n",
    "    model_weights_path = str(os.path.join(args['output_dir'], args['model'], f\"{args['model']}_model.pth\"))\n",
    "    model.load_state_dict(torch.load(model_weights_path, map_location=device))\n",
    "\n",
    "    # Set Criterion\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    criterion = criterion.to(device)\n",
    "\n",
    "    # Set Datasets\n",
    "    train_transform, test_transform = get_transforms(args['model'], args['dataset'])\n",
    "    _, test_loader = get_dataset(args['dataset'], args['data_dir'], train_transform, test_transform, args['batch_size'])\n",
    "\n",
    "    # Set Output Dir\n",
    "    os.makedirs(args['output_dir'], exist_ok=True)\n",
    "\n",
    "    # Test\n",
    "    test(args['model'], model, test_loader, criterion, device, args['output_dir'])\n"
   ],
   "id": "cc57b03ed1bc218f",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "TEST = True\n",
    "test_args = {\n",
    "    \"model\": \"ClassicNet\",\n",
    "    \"dataset\": \"FashionMNIST\",\n",
    "    \"data_dir\": \"../datasets\",\n",
    "    \"batch_size\": 64,\n",
    "    \"seed\": 42,\n",
    "    \"output_dir\": \"../output\",\n",
    "    \"device\": None,\n",
    "    \"qdevice\": \"default.qubit\",\n",
    "    \"qdevice_kwargs\": None,\n",
    "    \"diff_method\": \"best\"\n",
    "}\n",
    "if TEST:\n",
    "    main4test(test_args)"
   ],
   "id": "dbbd524feafe23b9",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 8. Predict",
   "id": "53ebe1d8887f564a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "515eefb8066902f8",
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "name": "python3",
   "language": "python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "colab": {
   "provenance": [],
   "gpuType": "L4",
   "collapsed_sections": [
    "6e40452d2a1605e",
    "a704979e47a8f8e",
    "e778e501589b5f0d",
    "6c602e95a7e9ebc2",
    "a96c2db88156aed2",
    "a021c57a3ddd6cd5"
   ]
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
